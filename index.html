<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">

    <title>YOLOv11 Autonomous Driving System | Final Year Project</title>

    <!-- Official description (used by Google, WhatsApp, QR previews) -->
    <meta name="description" content="YOLOv11-based autonomous driving system for real-time traffic sign detection and embedded deployment on Raspberry Pi 5.">

    <meta name="author" content="Ahmed Abd Elmoneim Mohammed">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" href="style.css">
</head>

<body>

<!-- ================= HERO ================= -->
<section class="hero">
    
    <!-- TOP HERO BUTTONS -->
    <div class="hero-actions">
        <a href="results.zip" class="hero-btn" download>Results</a>

        <a href="Yolov11_FYP_Report.pdf" class="hero-btn" target="_blank">
            Research Paper
        </a>

        <a href="https://wa.me/601111380724" class="hero-btn contact">
            Contact Me
        </a>
    </div>

    <!-- HERO CONTENT -->
    <div class="hero-content">
        <h1>YOLOv11 Autonomous Driving System</h1>

        <p class="hero-sub">
            A real-time vision-based autonomous driving system for intelligent traffic
            sign detection and autonomous navigation on embedded platforms.
        </p>

        <div class="hero-meta">
            <p><strong>Prepared by:</strong> Ahmed Abd Elmoneim Mohammed</p>
            <p><strong>Supervised by:</strong> Mr. Khairul Fikri Bin Muhammad</p>
            <p>Universiti Malaysia Pahang Al-Sultan Abdullah</p>
        </div>
    </div>

</section>


<!-- ================= INTRODUCTION ================= -->
<section class="content">
    <h2>General Introduction</h2>

    <p>
        This project presents the development of a YOLOv11-based autonomous driving system
        designed for real-time perception and navigation on embedded hardware. The system
        integrates a deep learning object detection model to enable efficient visual
        understanding of traffic environments. Visual input captured by an onboard camera
        is analysed to detect traffic signs and navigation cues. The detection outputs are
        translated into autonomous driving decisions such as stopping, turning, or
        continuing forward motion.
    </p>

    <div class="single-image">
        <img src="images/intro3.jpg" alt="System architecture and workflow diagram">
    </div>
</section>

<!-- ================= OBJECTIVES ================= -->
<section class="content">
    <h2>Objectives</h2>
    <ul>
        <li>To develop an embedded real-time object detection module using YOLOv11.</li>
        <li>To optimise YOLOv11 inference performance through NCNN deployment.</li>
        <li>To design an autonomous navigation mechanism informed by detection outputs.</li>
        <li>To evaluate system performance in terms of accuracy, latency, and navigation stability.</li>
    </ul>
</section>

<!-- ================= SCOPE ================= -->
<section class="content">
    <h2>Scope</h2>
    <ul>
        <li>The study focuses on developing an embedded autonomous driving system using YOLOv11 for real-time object detection.</li>
        <li>NCNN is utilised as the primary inference engine to optimise model performance on low-power hardware.</li>
        <li>The system includes camera-based perception, detection-driven navigation, and basic obstacle-avoidance capability.</li>
        <li>Testing is conducted within controlled indoor and semi-outdoor environments.</li>
    </ul>
</section>

<!-- ================= DELIMITATIONS ================= -->
<section class="content">
    <h2>Delimitations</h2>
    <ul>
        <li>The system does not incorporate GPS, LiDAR, radar, or multi-sensor fusion technologies.</li>
        <li>High-speed vehicular dynamics and full-scale automotive deployment are outside the study’s scope.</li>
        <li>Cloud-based processing and external GPU acceleration are excluded; all inference is performed locally.</li>
        <li>Long-range path planning and advanced SLAM algorithms are not implemented.</li>
        <li>Environmental robustness (e.g., rain, night-time driving, harsh terrain) is not addressed.</li>
    </ul>
</section>

<!-- ================= HARDWARE ================= -->
<section class="content">
    <h2>Hardware Components</h2>

    <div class="grid">
        <div class="grid-text">
            <h3>Raspberry Pi 5</h3>
            <p>
                The Raspberry Pi 5 serves as the central processing unit of the system. Compared to earlier
                Raspberry Pi versions, it offers improved CPU performance, higher memory bandwidth, and
                enhanced I/O capabilities. The 8GB RAM configuration provides sufficient memory for loading
                optimized deep learning models and handling image data during inference. This platform was
                selected due to its compatibility with the NCNN inference engine, enabling efficient execution
                without GPU acceleration.
            </p>
        </div>
        <img src="images/pi.jpg" alt="Raspberry Pi 5 hardware platform">
    </div>

    <div class="grid reverse">
        <img src="images/camera.jpg" alt="Raspberry Pi Camera Module">
        <div class="grid-text">
            <h3>Camera Module</h3>
            <p>
                The Pi Camera module is responsible for real-time image acquisition. Captured frames are
                processed by the YOLOv11 detection model, with resolution and frame rate configured to
                balance detection accuracy and inference speed.
            </p>
        </div>
    </div>

    <div class="grid">
        <div class="grid-text">
            <h3>Motor Driver and Motors</h3>
            <p>
                The Robot HAT functions as the hardware interface between the Raspberry Pi and the vehicle’s
                actuators. It manages motor control, power regulation, and peripheral connections, simplifying
                system integration and improving reliability.
            </p>
        </div>
        <img src="images/motor_driver.jpg" alt="Motor driver and actuator interface">
    </div>

    <div class="grid reverse">
        <img src="images/sensor.jpg" alt="Grayscale line detection sensor">
        <div class="grid-text">
            <h3>Grayscale Sensors</h3>
            <p>
                The grayscale sensor detects lane boundaries by measuring reflected light intensity. This
                sensor-based approach provides fast and deterministic feedback for navigation, complementing
                vision-based detection and reducing computational load.
            </p>
        </div>
    </div>
</section>

<!-- ================= VALIDATION ================= -->
<section class="content">
    <h2>Model Training and Validation Results</h2>

    <p>
        The YOLOv11-based traffic sign detection model was trained using a curated and annotated dataset.
        Training was conducted on Google Colab using GPU acceleration to ensure efficient convergence.
        Over multiple epochs, the model learned to accurately identify U-Turn, No Entry, Pedestrian Crossing,
        and Turn Left traffic signs, achieving a balance between detection accuracy and computational efficiency.
    </p>

    <div class="grid">
        <div class="grid-text">
            <h3>Training Overview</h3>
            <p>
                Training and validation losses decrease consistently, indicating stable learning behaviour
                without overfitting. High precision, recall, and mAP values demonstrate reliable detection
                performance suitable for embedded deployment.
            </p>
        </div>
        <img src="images/training_overview.jpg" alt="Training performance metrics">
    </div>

    <div class="grid">
        <div class="grid-text">
            <h3>Training Loss</h3>
            <p>
                Training loss decreases smoothly, reflecting effective bounding box regression and class
                discrimination.
            </p>
        </div>
        <img src="images/loss_train.jpg" alt="Training loss curve">
    </div>

    <div class="grid reverse">
        <img src="images/loss_val.jpg" alt="Validation loss curve">
        <div class="grid-text">
            <h3>Validation Loss</h3>
            <p>
                Validation loss closely follows training loss, confirming good generalisation to unseen data.
            </p>
        </div>
    </div>
</section>

<!-- ================= RESULTS ================= -->
<section class="content">
    <h2>Visual Detection Results</h2>

    <p>
        Visual examination confirms that the trained model reliably identifies and classifies traffic signs
        in real time. Bounding boxes are correctly localised and labelled with high confidence scores.
    </p>

    <ul>
        <li>U-Turn signs detected with confidence scores up to <strong>0.98</strong></li>
        <li>No Entry signs detected with confidence scores up to <strong>0.97</strong></li>
        <li>Pedestrian Crossing signs detected with confidence scores up to <strong>0.97</strong></li>
        <li>Turn Left signs detected with confidence scores up to <strong>0.97</strong></li>
    </ul>

    <div class="single-image">
        <img src="images/visual_all_signs.jpg" alt="Detection of multiple traffic signs">
        <p class="caption"><strong>Figure:</strong> Detection of multiple traffic signs with correct localisation.</p>
    </div>

    <div class="results-grid">
        <div>
            <img src="images/visual_noentry.jpg" alt="No Entry sign detection">
            <p><strong>No Entry Sign</strong></p>
        </div>

        <div>
            <img src="images/visual_uturn.jpg" alt="U-Turn sign detection">
            <p><strong>U-Turn Sign</strong></p>
        </div>

        <div>
            <img src="images/visual_pedestrian.jpg" alt="Pedestrian Crossing sign detection">
            <p><strong>Pedestrian Crossing Sign</strong></p>
        </div>

        <div>
            <img src="images/visual_turnleft.jpg" alt="Turn Left sign detection">
            <p><strong>Turn Left Sign</strong></p>
        </div>
    </div>
</section>

<section class="content">
    <h2>Project Information</h2>
    <ul>
        <li><strong>Project Type:</strong> Final Year Project (FYP)</li>
        <li><strong>Field:</strong> Embedded Systems & Autonomous Robotics</li>
        <li><strong>Institution:</strong> Universiti Malaysia Pahang Al-Sultan Abdullah</li>
        <li><strong>Year:</strong> 2026</li>
    </ul>
</section>

<footer>
    <p>
        © 2026 Ahmed Abd Elmoneim Mohammed<br>
        Final Year Project (FYP)<br>
        Universiti Malaysia Pahang Al-Sultan Abdullah
    </p>
</footer>

</body>
</html>
